import asyncio
import os
import re
import urllib.parse
import aiohttp
import csv
import playwright
from pathlib import Path
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# CSVè®°å½•æ–‡ä»¶è·¯å¾„
RECORD_FILE = "scraping_records.csv"

# åˆæ³•åŒ–æ–‡ä»¶å
def sanitize_filename(name: str) -> str:
    name = re.sub(r'[\\/*?:"<>|]', '', name)
    return name.strip()[:100]

# åŠ è½½å·²æŠ“å–çš„è®°å½•
def load_scraped_records() -> set:
    records = set()
    if Path(RECORD_FILE).exists():
        with open(RECORD_FILE, 'r', encoding='utf-8', newline='') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['status'] == '1':  # åªè·³è¿‡æˆåŠŸæŠ“å–çš„
                    records.add(row['title'])
    return records

# è®°å½•æŠ“å–çŠ¶æ€
def record_scraping_status(title: str, status: int, error_msg: str = ""):
    file_exists = Path(RECORD_FILE).exists()
    with open(RECORD_FILE, 'a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(['title', 'status', 'error_message'])
        writer.writerow([title, status, error_msg])

# å¤„ç†å›¾ç‰‡æ–‡ä»¶åå†²çª
def resolve_duplicate_filename(base_path: Path) -> Path:
    if not base_path.exists():
        return base_path
    i = 1
    while True:
        new_path = base_path.with_name(f"{base_path.stem}.{i}{base_path.suffix}")
        if not new_path.exists():
            return new_path
        i += 1

# ç›´æ¥è¿”å›HTMLï¼Œä¸å¤„ç†å›¾ç‰‡
def process_html(html: str) -> str:
    return html

early_stop = False
# ä¸»ç¨‹åº
async def scrape_all_articles():
    # ä¸å†éœ€è¦åˆ›å»ºimgç›®å½•ï¼Œå› ä¸ºä¸ä¸‹è½½å›¾ç‰‡
    Path("data").mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½å·²æŠ“å–çš„è®°å½•
    scraped_records = load_scraped_records()
    print(f"ğŸ“‹ å·²åŠ è½½ {len(scraped_records)} æ¡æŠ“å–è®°å½•")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        # åˆ›å»ºä¸Šä¸‹æ–‡æ—¶ç¦ç”¨å›¾ç‰‡åŠ è½½
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )
        page = await context.new_page()
        
        # ç¦ç”¨å›¾ç‰‡åŠ è½½
        await page.route("**/*.{png,jpg,jpeg,gif,svg,webp}", lambda route: route.abort())

        await page.goto("https://csprojectedu.com/")
        
        # è¿ç»­è·³è¿‡è®¡æ•°å™¨
        consecutive_skipped = 0
        max_consecutive_skipped = 5
        
        while True:
            article_links = await page.locator("//a[@class='post-title-link']").all()
            print(f"ğŸ“„ Found {len(article_links)} articles.")

            for i in range(len(article_links)):
                article_links = await page.locator("//a[@class='post-title-link']").all()
                link = article_links[i]

                raw_title = await link.inner_text()
                clean_title = sanitize_filename(raw_title)
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»æŠ“å–è¿‡
                if clean_title in scraped_records:
                    consecutive_skipped += 1
                    print(f"â­ï¸ è·³è¿‡å·²æŠ“å–çš„æ–‡ç« : {clean_title} (è¿ç»­è·³è¿‡: {consecutive_skipped})")
                    
                    # æ£€æŸ¥æ˜¯å¦è¿ç»­è·³è¿‡æ¬¡æ•°è¾¾åˆ°é˜ˆå€¼
                    if early_stop and consecutive_skipped >= max_consecutive_skipped:
                        print(f"ğŸ¯ è¿ç»­è·³è¿‡ {max_consecutive_skipped} ç¯‡æ–‡ç« ï¼Œæ¨æ–­å·²æŠ“å–å®Œæ‰€æœ‰æ›´æ–°å†…å®¹")
                        print("ğŸš€ ç¨‹åºæå‰é€€å‡ºï¼Œæ‰€æœ‰æ–°å†…å®¹å·²æŠ“å–å®Œæ¯•ï¼")
                        await browser.close()
                        return
                    continue
                
                # é‡ç½®è¿ç»­è·³è¿‡è®¡æ•°å™¨ï¼ˆå‘ç°æ–°æ–‡ç« ï¼‰
                consecutive_skipped = 0
                save_path = Path("data") / f"{clean_title}.html"

                print(f"â¡ï¸ Opening article: {clean_title}")
                
                try:
                    await asyncio.sleep(1)
                    await link.click()
                    await page.wait_for_selector('//*[@id="posts"]/article')

                    article_elem = await page.locator('//*[@id="posts"]/article').element_handle()
                    html = await article_elem.inner_html()

                    # å¤„ç†HTMLï¼ˆä¸ä¸‹è½½å›¾ç‰‡ï¼‰
                    updated_html = process_html(html)

                    # ä¿å­˜ HTML
                    save_path.write_text(updated_html, encoding='utf-8')
                    print(f"âœ… HTML saved to {save_path}")
                    
                    # è®°å½•æˆåŠŸçŠ¶æ€
                    record_scraping_status(clean_title, 1)
                    print(f"ğŸ“ è®°å½•æˆåŠŸçŠ¶æ€: {clean_title}")

                except Exception as e:
                    error_msg = str(e)
                    print(f"âŒ æŠ“å–å¤±è´¥: {clean_title}, é”™è¯¯: {error_msg}")
                    # è®°å½•å¤±è´¥çŠ¶æ€
                    record_scraping_status(clean_title, 0, error_msg)

                await asyncio.sleep(1)
                try:
                    await page.go_back()
                except playwright._impl._errors.TimeoutError:
                    print("â—ï¸ è¿”å›ä¸Šä¸€é¡µå¤±è´¥ï¼Œå¯èƒ½æ˜¯é¡µé¢æŸäº›ç»„ä»¶åŠ è½½éå¸¸ç¼“æ…¢å¯¼è‡´é—®é¢˜")
                    continue
                await page.wait_for_selector("//a[@class='post-title-link']")

            next_button = page.locator("//a[@class='extend next']")
            if await next_button.count() == 0 or not await next_button.is_visible():
                print("ğŸ No more pages.")
                break
            else:
                await asyncio.sleep(1)
                await next_button.click()
                await page.wait_for_selector("//a[@class='post-title-link']")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(scrape_all_articles())
