https://x.com/_avichawla/status/1896810340991803443

Here's an overview of our app:

• 1) Takes an audio file and transcribes it using @AssemblyAI

To transcribe audio files, get an API key from AssemblyAI and store it in the `.env` file。

We use AssemblyAI to transcribe audio with speaker labels. To do this: - We set up the transcriber object. - We enable speaker label detection in the config. - We transcribe the audio using AssemblyAI. （图二code）



• 2-3) Stores it in a Qdrant vector database.

To do this, we: 

Load the embedding model and generate embeddings. 

Connect to Qdrant and create a collection.
Store the embeddings（图三）

• 4-6) Queries the database to get context.

Now, we query the vector database to retrieve sentences in the transcripts that are similar to the query: 

Convert the query into an embedding. （图四）

Search the vector database. 

Retrieve the top results.

• 7-8) Uses DeepSeek-R1 as the LLM to generate a response.

Finally, after retrieving the context:

We construct a prompt.

We use DeepSeek-R1 through @ollama to generate a response.（图五）





